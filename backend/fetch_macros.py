
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
macros_scraper_v10e.py

Changes vs v10c:
- Uses the item IDs from your menu_data.json when inserting into `public.macros.id`
  (table is `GENERATED BY DEFAULT AS IDENTITY`, so explicit IDs are allowed).
- If a row with that ID already exists, we **skip** (no update).
- This means IDs can be sparse (gaps) on days a given item doesn't appearâ€”exactly as requested.

Other features preserved:
- Harvest ALL categories for each meal on longmenu pages.
- URL cleaning to avoid Selenium invalid-argument errors.
- Tolerant matching (exact -> contains -> fuzzy) from JSON names to page titles.
- Robust label parsing, FDA disclaimer removed.

Env vars:
  MENU_JSON (default: menu_data.json)
  IS_HEADLESS=1 (default) or 0
  SUPABASE_URL, SUPABASE_SERVICE_KEY
  SUPABASE_TABLE (default: macros)
"""
import os, re, time, json, argparse
from difflib import SequenceMatcher
from urllib.parse import urlparse, urljoin, urlsplit, urlunsplit, parse_qsl, urlencode, quote

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, StaleElementReferenceException, ElementClickInterceptedException

from bs4 import BeautifulSoup, NavigableString, Tag
from supabase import create_client, Client

# -----------------------------
# Config / constants
# -----------------------------
DEFAULT_JSON_PATH = os.environ.get("MENU_JSON", "menu_data.json")
IS_HEADLESS = os.environ.get("IS_HEADLESS", "1") == "1"

SUPABASE_URL = os.environ.get("SUPABASE_URL", "https://jbvsfjuufpoohaimookq.supabase.co")
SUPABASE_KEY = os.environ.get("SUPABASE_SERVICE_KEY", "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6ImpidnNmanV1ZnBvb2hhaW1vb2txIiwicm9sZSI6InNlcnZpY2Vfcm9sZSIsImlhdCI6MTc2MjM3OTUyNiwiZXhwIjoyMDc3OTU1NTI2fQ.Llz8ROtP2ohe7rdO2sYtIlaufPNrvHLAJa43r2yPL2U")  # service_role key (server-side only)

TABLE_NAME = os.environ.get("SUPABASE_TABLE", "macros")

BASE_LOCATIONS_URL = "https://nutrition.sa.ucsc.edu/location.aspx"

WS = re.compile(r"\s+", re.UNICODE)

def norm(s: str) -> str:
    s = (s or "").replace("\xa0", " ").replace("&nbsp;", " ").replace("&amp;", "&")
    return WS.sub(" ", s).strip()

def norm_text(s: str) -> str:
    if not s: return ""
    s = s.lower().replace("&amp;", "&")
    s = re.sub(r"[\/&]", " and ", s)
    s = re.sub(r"[^a-z0-9\s]", " ", s)
    s = re.sub(r"\b(dining\s*hall|hall|dining)\b", " ", s)
    s = WS.sub(" ", s).strip()
    return s

# ---- URL cleaner to avoid Selenium invalid-argument ----
def clean_url(current_url: str, href: str) -> str | None:
    if not href: return None
    href = href.strip()
    if href.startswith(("javascript:", "mailto:", "#")):
        return None
    absu = urljoin(current_url, href)
    parts = urlsplit(absu)
    path = quote(parts.path, safe="/:@%+~")
    query = urlencode(parse_qsl(parts.query, keep_blank_values=True))
    cleaned = urlunsplit((parts.scheme, parts.netloc, path, query, parts.fragment))
    if not cleaned.lower().startswith(("http://", "https://")):
        return None
    return cleaned

# ---- Halls ----
HALL_ALIASES = {
    "cowell and stevenson": ["cowell/stevenson", "cowell stevenson", "cowell and stevenson"],
    "crown and merrill": ["crown/merrill", "crown merrill", "crown and merrill"],
    "banana joes": ["banana joe's", "banana joes"],
    "john r lewis and college nine": [
        "john r. lewis", "college nine", "college nine and john r. lewis",
        "john r lewis college nine", "john r lewis & college nine"
    ],
}
def expand_aliases(q: str) -> list[str]:
    base = norm_text(q)
    alts = set([base])
    for canon, variants in HALL_ALIASES.items():
        if canon in base or base in canon:
            alts.add(canon)
            for v in variants: alts.add(norm_text(v))
    return list(alts)

def token_subset(a: str, b: str) -> bool:
    ta = set(norm_text(a).split()); tb = set(norm_text(b).split())
    return bool(ta) and (ta.issubset(tb) or tb.issubset(ta))

def fuzzy_str(a: str, b: str) -> float:
    return SequenceMatcher(None, norm_text(a), norm_text(b)).ratio()

# ---- Macros & meals ----
NAMES = {
    "total_fat": ["Total Fat"],
    "sat_fat": ["Saturated Fat", "Sat Fat", "Sat. Fat", "Saturated Fatty Acids"],
    "trans_fat": ["Trans Fat", "Trans"],
    "cholesterol": ["Cholesterol"],
    "sodium": ["Sodium"],
    "total_carb": [
        "Total Carbohydrate", "Total Carbohydrates", "Carbohydrate", "Carbohydrates",
        "Total Carb", "Tot. Carb.", "Total Carbs"
    ],
    "dietary_fiber": ["Dietary Fiber", "Fiber"],
    "sugars": ["Total Sugars", "Sugars"],
    "protein": ["Protein"],
}
MICROS = ["Vitamin D", "Calcium", "Iron", "Potassium"]

MEAL_NAME_NORMALIZE = {
    "breakfast": "Breakfast",
    "lunch": "Lunch",
    "dinner": "Dinner",
    "brunch": "Brunch",
}

# ---- Selenium helpers ----
def make_driver():
    chrome_opts = Options()
    if IS_HEADLESS:
        chrome_opts.add_argument("--headless=new")
    chrome_opts.add_argument("--no-sandbox")
    chrome_opts.add_argument("--disable-dev-shm-usage")
    chrome_opts.add_argument("--user-agent=Mozilla/5.0")
    return webdriver.Chrome(options=chrome_opts)

def wait_for_links(driver, timeout=15):
    WebDriverWait(driver, timeout).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, "a")))

def get_hall_link(driver, hall_name: str) -> str | None:
    wait_for_links(driver, 15)
    anchors = driver.find_elements(By.CSS_SELECTOR, "a")
    cand = []
    for a in anchors:
        raw_href = a.get_attribute("href") or ""
        href = clean_url(driver.current_url, raw_href)
        txt = a.text.strip()
        if href and txt: cand.append((txt, href))
    if not cand: return None
    queries = expand_aliases(hall_name)
    for q in queries:
        qn = norm_text(q)
        for txt, href in cand:
            if norm_text(txt) == qn: return href
    for q in queries:
        for txt, href in cand:
            if token_subset(q, txt): return href
    best = None; best_score = 0.0
    for q in queries:
        for txt, href in cand:
            score = fuzzy_str(q, txt)
            if score > best_score:
                best, best_score = href, score
    return best if best_score >= 0.75 else None

def apply_params(base_url: str, date_str: str | None, meal_name: str | None) -> str:
    if not base_url: return base_url
    p = urlparse(base_url); q = dict(parse_qsl(p.query, keep_blank_values=True))
    if date_str: q["dtdate"] = date_str
    if meal_name: q["mealName"] = meal_name
    return urlunsplit((p.scheme, p.netloc, p.path, urlencode(q), p.fragment))

def find_nutrient_calc_href(driver, timeout=10) -> str | None:
    wait_for_links(driver, timeout)
    for a in driver.find_elements(By.CSS_SELECTOR, "a"):
        text = (a.text or "").strip().lower()
        if "nutrition calculator" in text:
            href = clean_url(driver.current_url, a.get_attribute("href") or "")
            if href: return href
    return None

def expand_all_categories(driver):
    selectors = [
        "a[onclick*='Expand']", "a[onclick*='expand']", "a[href*='Expand']", "a[href*='expand']",
        "input[value*='Expand']", "input[value*='Show']", "img[alt*='Expand']", "img[alt*='Show']",
        "a[id*='lnkShow']", "a[id*='Show']", "span[onclick*='Expand']", "span[onclick*='expand']",
    ]
    for sel in selectors:
        try:
            elems = driver.find_elements(By.CSS_SELECTOR, sel)
            for el in elems:
                try:
                    driver.execute_script("arguments[0].scrollIntoView({block:'center'});", el)
                    time.sleep(0.05)
                    el.click()
                except (StaleElementReferenceException, ElementClickInterceptedException):
                    try:
                        driver.execute_script("arguments[0].click();", el)
                    except Exception:
                        pass
        except Exception:
            pass
    driver.execute_script("""
        (function(){
            const sels = ['div','table','tbody','tr','ul','li','p'];
            for (const s of sels) {
                document.querySelectorAll(s).forEach(el => {
                    const cs = window.getComputedStyle(el);
                    if (cs && (cs.display === 'none' || el.hidden)) {
                        el.style.display = 'block';
                        el.hidden = false;
                    }
                });
            }
        })();
    """)

def get_all_item_links_from_longmenu(driver, timeout=20) -> dict[str, str]:
    wait_for_links(driver, timeout)
    expand_all_categories(driver)
    # Scroll until the number of item links stops increasing (catches lazy rendering)
    prev = -1
    for _ in range(16):
        links = driver.find_elements(By.CSS_SELECTOR, "a[href*='label.aspx'][href*='RecNumAndPort']")
        count = len(links)
        if count == prev:
            break
        prev = count
        driver.execute_script("window.scrollBy(0, document.body.scrollHeight/2);")
        time.sleep(0.12)
    driver.execute_script("window.scrollTo(0, 0);")
    time.sleep(0.08)

    # Parse final DOM to harvest all anchors
    soup = BeautifulSoup(driver.page_source, "html.parser")
    items = {}
    base = driver.current_url
    for a in soup.select("a[href*='label.aspx'][href*='RecNumAndPort']"):
        raw = a.get("href") or ""
        href = clean_url(base, raw)
        text = norm(a.get_text(" ", strip=True))
        if href and text:
            items[text] = href
    return items

# -----------------------------
# Parsing helpers (unchanged from v10c)
# -----------------------------
def extract_serving_size(soup: BeautifulSoup) -> str | None:
    node = soup.find(string=re.compile(r"\bServing\s*Size\b", re.I))
    if node:
        def collect_after(tag: Tag):
            parts = []
            for sib in tag.next_siblings:
                if isinstance(sib, Tag) and sib.name.lower() == "br":
                    break
                txt = sib.get_text(" ", strip=True) if isinstance(sib, Tag) else str(sib)
                if txt: parts.append(norm(txt))
            return norm(" ".join(parts))
        if isinstance(node, NavigableString) and isinstance(node.parent, Tag):
            v = collect_after(node.parent); 
            if v and not re.search(r"amount\s*/\s*serving", v, re.I): return v
        par = node.parent
        if par and par.parent and isinstance(par.parent, Tag):
            v = collect_after(par.parent)
            if v and not re.search(r"amount\s*/\s*serving", v, re.I): return v
    for td in soup.find_all(["td","th"]):
        if re.search(r"\bServing\s*Size\b", td.get_text(" ", strip=True), re.I):
            sib = td.find_next_sibling(["td","th"])
            if sib:
                v = norm(sib.get_text(" ", strip=True))
                if v and not re.search(r"amount\s*/\s*serving", v, re.I): return v
    txt = soup.get_text(" ", strip=True)
    m = re.search(r"Serving\s*Size\s*[:\-]?\s*(.+?)(?:Servings|$)", txt, re.I)
    if m:
        v = norm(m.group(1))
        if not re.search(r"amount\s*/\s*serving", v, re.I): return v
    return None

def is_macro_label(lbl: str) -> str | None:
    l = lbl.lower()
    for key, variants in NAMES.items():
        for v in variants:
            if v.lower() in l: return key
    return None

def parse_pair_cell(td: Tag) -> tuple[str|None, str|None]:
    raw = norm(td.get_text(" ", strip=True))
    b = td.find("b")
    if b:
        label = norm(b.get_text(" ", strip=True))
        def collect_after(node: Tag):
            parts = []
            for sib in node.next_siblings:
                if isinstance(sib, Tag) and sib.name.lower() == "hr": break
                txt = sib.get_text(" ", strip=True) if isinstance(sib, Tag) else str(sib)
                if txt: parts.append(norm(txt))
            return norm(" ".join(parts))
        trailing = collect_after(b)
        if not trailing and isinstance(b.parent, Tag):
            trailing = collect_after(b.parent)
        m = re.search(r"([0-9]+(?:\.[0-9]+)?\s*[a-zA-Z%]+)", trailing)
        amount = None
        if m: amount = re.sub(r"\s+(?=[a-zA-Z%])", "", m.group(1))
        return (label, amount if amount else None)
    m = re.search(r"^(.*?)[\s:]*([0-9]+(?:\.[0-9]+)?\s*[a-zA-Z%]+)\s*$", raw)
    if m:
        label = norm(m.group(1))
        amount = re.sub(r"\s+(?=[a-zA-Z%])", "", norm(m.group(2)))
        return (label, amount)
    return (raw if raw else None, None)

def parse_label_html(html: str) -> dict:
    soup = BeautifulSoup(html, "html.parser")
    page_text = soup.get_text(" ", strip=True)

    # Strip FDA disclaimer
    page_text = re.sub(
        r"The nutrient composition of food may vary.*?www\.cfsan\.fda\.gov/~dms/foodlab\.html",
        "",
        page_text,
        flags=re.I | re.S
    )

    # Name
    name = None
    for sel in [".labelrecipes span", ".shortmenurecipes span", "div.srtitle span", "div.srtitle", "h1", "h2"]:
        el = soup.select_one(sel)
        if el:
            name = norm(el.get_text(" ", strip=True))
            if name: break

    # Serving size & calories
    serving_size = extract_serving_size(soup)

    calories = None
    m = re.search(r"Calories\s*[:\-]?\s*([0-9]+)", page_text, re.I)
    if m: calories = m.group(1)

    # Macros
    nutrients = {k: None for k in NAMES}
    for tr in soup.find_all("tr"):
        tds = tr.find_all("td")
        if len(tds) == 4:
            left_label, left_amt = parse_pair_cell(tds[0])
            left_dv_raw = tds[1].get_text(" ", strip=True)
            left_dv = None; mm = re.search(r"([0-9]{1,3})\s*%?", left_dv_raw)
            if mm: left_dv = mm.group(1) + "%"
            if left_label:
                k = is_macro_label(left_label)
                if k and nutrients[k] is None:
                    nutrients[k] = {"amount": left_amt, "dv": left_dv}
            right_label, right_amt = parse_pair_cell(tds[2])
            right_dv_raw = tds[3].get_text(" ", strip=True)
            right_dv = None; mm = re.search(r"([0-9]{1,3})\s*%?", right_dv_raw)
            if mm: right_dv = mm.group(1) + "%"
            if right_label:
                k = is_macro_label(right_label)
                if k and nutrients[k] is None:
                    nutrients[k] = {"amount": right_amt, "dv": right_dv}
        elif len(tds) == 2:
            label, amt = parse_pair_cell(tds[0])
            dv_raw = tds[1].get_text(" ", strip=True)
            dv = None; mm = re.search(r"([0-9]{1,3})\s*%?", dv_raw)
            if mm: dv = mm.group(1) + "%"
            if label:
                k = is_macro_label(label)
                if k and nutrients[k] is None:
                    nutrients[k] = {"amount": amt, "dv": dv}

    # Backfill macros via regex if still missing
    def sweep(variants):
        for v in variants:
            m = re.search(
                rf"{re.escape(v)}\s*[:\-]?\s*([0-9]+(?:\.[0-9]+)?\s*[a-zA-Z%]+)",
                page_text, re.I
            )
            if m:
                amount = re.sub(r"\s+(?=[a-zA-Z%])", "", norm(m.group(1)))
                tail = page_text[m.end(): m.end() + 30]
                mdv = re.search(r"([0-9]{1,3})\s*%?", tail)
                dv = mdv.group(1)+"%" if mdv else None
                return amount, dv
        return None, None

    for key, variants in NAMES.items():
        if nutrients[key] is None:
            amount, dv = sweep(variants)
            if amount or dv:
                nutrients[key] = {"amount": amount, "dv": dv}

    # Micronutrients
    micro_values = {m.lower(): None for m in MICROS}

    for micro in MICROS:
        node = soup.find(string=re.compile(rf"\b{re.escape(micro)}\b", re.I))
        amt_or_dv = None
        if node:
            parent = node.parent if isinstance(node, NavigableString) else None
            if parent and isinstance(parent, Tag):
                sib_fonts = parent.find_next_siblings("font")
                val_text = " ".join(norm(f.get_text(' ', strip=True)) for f in sib_fonts) if sib_fonts else ""
                if not val_text:
                    nf = parent.find_next("font")
                    if nf:
                        val_text = norm(nf.get_text(" ", strip=True))
                mdv = re.search(r"([0-9]{1,3})\s*%?", val_text)
                if mdv:
                    amt_or_dv = mdv.group(1) + "%"
                else:
                    mamt = re.search(r"([0-9]+(?:\.[0-9]+)?\s*[a-zA-Z]+)", val_text)
                    if mamt:
                        amt_or_dv = re.sub(r"\s+(?=[a-zA-Z%])", "", mamt.group(1))

        if not amt_or_dv:
            mnear = re.search(
                rf"{re.escape(micro)}.*?([0-9]+(?:\.[0-9]+)?\s*[a-zA-Z]+|[0-9]{{1,3}}\s*%?)",
                page_text, re.I
            )
            if mnear:
                token = norm(mnear.group(1))
                if re.fullmatch(r"[0-9]{1,3}\s*%?", token):
                    token = re.sub(r"\s*", "", token)
                    if not token.endswith("%"):
                        token += "%"
                else:
                    token = re.sub(r"\s+(?=[a-zA-Z%])", "", token)
                amt_or_dv = token

        micro_values[micro.lower()] = amt_or_dv

    record = {
        "name": name,
        "serving_size": serving_size,
        "calories": calories,
        "total_fat": nutrients["total_fat"],
        "sat_fat": nutrients["sat_fat"],
        "trans_fat": nutrients["trans_fat"],
        "cholesterol": nutrients["cholesterol"],
        "sodium": nutrients["sodium"],
        "total_carb": nutrients["total_carb"],
        "dietary_fiber": nutrients["dietary_fiber"],
        "sugars": nutrients["sugars"],
        "protein": nutrients["protein"],
        "vitamin_d": micro_values["vitamin d"],
        "calcium": micro_values["calcium"],
        "iron": micro_values["iron"],
        "potassium": micro_values["potassium"],
        "ingredients": None,
        "allergens": None,
    }

    m = re.search(r"Ingredients?:\s*(.+?)(?:Allergens?:|Contains:|$)", page_text, re.I)
    if m: record["ingredients"] = norm(m.group(1))
    m = re.search(r"(Allergens?|Contains):\s*(.+?)(?:\s{2,}|$)", page_text, re.I)
    if m: record["allergens"] = norm(m.group(2))

    return record

# -----------------------------
# Supabase + runner
# -----------------------------
def supabase_client() -> Client | None:
    if not (SUPABASE_URL and SUPABASE_KEY):
        return None
    return create_client(SUPABASE_URL, SUPABASE_KEY)

def insert_with_explicit_id_or_skip(sb: Client, desired_id: int, record: dict, dry_run=False, debug=False):
    """Insert the row with the explicit id. If that id exists already, skip (no update)."""
    # Guard: ensure id isn't in payload unless we're inserting
    payload = dict(record)
    payload["id"] = desired_id

    if dry_run or sb is None:
        print(f"  [dry-run] would insert id={desired_id} name={record.get('name')}")
        return

    try:
        sel = sb.table(TABLE_NAME).select("id").eq("id", desired_id).limit(1).execute()
        rows = sel.data or []
        if rows:
            print(f"  [skip] id={desired_id} exists (name={record.get('name')})")
            return
        sb.table(TABLE_NAME).insert(payload).execute()
        print(f"  [insert:id] {record.get('name')} (id={desired_id})")
    except Exception as e:
        print(f"  [db-error] id={desired_id} name={record.get('name')}: {e}")

def load_menu_by_hall_and_meal(json_path: str, hall_filter: str | None):
    """
    Return mapping:
      { hall: { meal: { norm_name: {"id": <int>, "name": <orig>} } } }
    Assumes menu_data.json structure:
      { "halls": { "<hall>": { "<meal>": { "<section>": [ {"id": ..., "name": ...}, ... ] } } } }
    """
    with open(json_path, "r", encoding="utf-8") as f:
        data = json.load(f)
    halls = data.get("halls", {})
    result = {}
    for hall, meals in halls.items():
        if hall_filter and norm(hall) != norm(hall_filter):
            continue
        per_meal = {}
        for meal_name, sections in (meals or {}).items():
            per_meal_map = {}
            for subsection, items in (sections or {}).items():
                for item in (items or []):
                    nm = item.get("name")
                    iid = item.get("id")
                    if nm and iid is not None:
                        per_meal_map[norm(nm)] = {"id": int(iid), "name": nm}
            if per_meal_map:
                per_meal[meal_name] = per_meal_map
        if per_meal:
            result[hall] = per_meal
    return result

# ---- tolerant matching helpers ----
def normalize_item_title(s: str) -> str:
    s = (s or "")
    s = s.replace("\xa0", " ").replace("&nbsp;", " ")
    s = re.sub(r"\s+", " ", s).strip()
    # Drop dietary flags and trailing descriptors
    s = re.sub(r"\((?:V|VG|VEGAN|GF|GFO|DF|HALAL|KOSHER|VEGETARIAN)\)", "", s, flags=re.I)
    s = re.sub(r"\s+with\s+.+$", "", s, flags=re.I)
    s = re.sub(r"\s+w\/\s+.+$", "", s, flags=re.I)
    return s.lower()

def fuzzy_title_score(a: str, b: str) -> float:
    return SequenceMatcher(None, normalize_item_title(a), normalize_item_title(b)).ratio()

def run(hall_filter: str | None, date_override: str | None, limit: int | None, dry_run: bool, debug: bool, harvest_all: bool):
    menu_path = DEFAULT_JSON_PATH
    if not os.path.exists(menu_path):
        raise FileNotFoundError(f"MENU_JSON not found: {menu_path}")
    hall_to_meals = load_menu_by_hall_and_meal(menu_path, hall_filter)
    if not hall_to_meals:
        print("No halls or names found in menu_data.json for the given filter.")
        return

    sb = supabase_client()
    if sb is None and not dry_run:
        print("[warn] Supabase credentials not set. Using --dry-run mode.")
        dry_run = True

    driver = make_driver()
    try:
        driver.get(BASE_LOCATIONS_URL)
        wait_for_links(driver)

        for hall, meals in hall_to_meals.items():
            print(f"\n=== Hall: {hall} ===")
            driver.get(BASE_LOCATIONS_URL); wait_for_links(driver)
            hall_href = get_hall_link(driver, hall)
            if not hall_href:
                print(f"  [warn] Hall link not found; skipping {hall}")
                continue

            driver.get(hall_href)
            calc_href = find_nutrient_calc_href(driver)
            if not calc_href:
                print("  [warn] 'Nutrient Calculator' not found; skipping hall")
                driver.get(BASE_LOCATIONS_URL); wait_for_links(driver)
                continue

            for meal_key, names_map in meals.items():
                normalized_meal = MEAL_NAME_NORMALIZE.get(meal_key.lower().strip(), meal_key.title().strip())
                longmenu_url = apply_params(calc_href, date_override, normalized_meal)
                print(f"  -> Meal: {normalized_meal}")
                driver.get(longmenu_url)

                try:
                    item_links = get_all_item_links_from_longmenu(driver)
                except TimeoutException:
                    print(f"  [warn] longmenu page didn't load links for meal {normalized_meal}; skipping meal")
                    continue

                if not item_links:
                    print("  [warn] No items found on longmenu page.")
                    continue

                # Optional: harvest everything on the page (assign no external IDs here)
                if harvest_all:
                    processed = 0
                    for title, href in item_links.items():
                        safe_href = clean_url(driver.current_url, href)
                        if not safe_href:
                            if debug: print(f"  [skip-url] {title}: {href}")
                            continue
                        driver.get(safe_href)
                        try:
                            WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, "body")))
                        except TimeoutException:
                            print(f"  [timeout] {title}: label page"); continue
                        html = driver.page_source
                        record = parse_label_html(html)
                        if not record.get("name"):
                            record["name"] = title
                        # In harvest-all, we cannot infer external ID: insert without forcing id
                        if sb is None or dry_run:
                            print(f"  [dry-run] would insert (no external id): {record.get('name')}")
                        else:
                            try:
                                sb.table(TABLE_NAME).insert(record).execute()
                                print(f"  [insert] {record.get('name')} (auto id)")
                            except Exception as e:
                                print(f"  [db-error] (auto id) {record.get('name')}: {e}")
                        processed += 1
                        if limit and processed >= limit:
                            print(f"  [limit] processed {limit} items for {normalized_meal}")
                            break
                    continue  # next meal

                # Build indices for tolerant matching on this meal page
                # Map normalized title -> href
                norm_index = {normalize_item_title(t): h for t, h in item_links.items()}

                count = 0
                # names_map: { norm(original_name): {"id": id, "name": original_name} }
                for nm_norm_key, meta in names_map.items():
                    desired_id = meta["id"]
                    json_name = meta["name"]

                    # First try exact normalized match
                    href = norm_index.get(normalize_item_title(json_name))

                    # 2) contains / contained-in
                    if not href:
                        for title, h in item_links.items():
                            nt = normalize_item_title(title)
                            tgt = normalize_item_title(json_name)
                            if tgt in nt or nt in tgt:
                                href = h; break

                    # 3) fuzzy fallback
                    if not href:
                        best, best_score = None, 0.0
                        for title, h in item_links.items():
                            score = fuzzy_title_score(title, json_name)
                            if score > best_score:
                                best, best_score = h, score
                        if best_score >= 0.82:
                            href = best

                    if not href:
                        continue

                    safe_href = clean_url(driver.current_url, href)
                    if not safe_href:
                        if debug: print(f"  [skip-url] Bad href for {json_name}: {href}")
                        continue

                    driver.get(safe_href)
                    try:
                        WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, "body")))
                    except TimeoutException:
                        print(f"  [timeout] {json_name}: label page")
                        continue

                    html = driver.page_source
                    record = parse_label_html(html)
                    if not record.get("name"):
                        record["name"] = json_name

                    insert_with_explicit_id_or_skip(sb, desired_id, record, dry_run=dry_run, debug=debug)

                    count += 1
                    if limit and count >= limit:
                        print(f"  [limit] processed {limit} items for {normalized_meal}")
                        break

                    time.sleep(0.2)

            print(f"=== Finished {hall} ===")
            driver.get(BASE_LOCATIONS_URL); wait_for_links(driver)

    finally:
        driver.quit()

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--hall", type=str, default=None, help="Only scrape this hall name")
    ap.add_argument("--date", type=str, default=None, help="Override dtdate (e.g., 11/09/2025)")
    ap.add_argument("--limit", type=int, default=None, help="Limit items per meal (or harvest count in --harvest-all)")
    ap.add_argument("--dry-run", action="store_true", help="Parse but do not write to DB")
    ap.add_argument("--debug", action="store_true", help="Print parsed records to stdout")
    ap.add_argument("--harvest-all", action="store_true", help="Scrape ALL found items for each meal (ignore JSON filter)")
    args = ap.parse_args()
    run(args.hall, args.date, args.limit, args.dry_run, args.debug, args.harvest_all)

if __name__ == "__main__":
    main()
